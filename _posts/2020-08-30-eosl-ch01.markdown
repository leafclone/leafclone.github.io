---
layout: post
mathjax: true
title:  "Elements of Statistical Learning chapter 2"
date:   2020-08-30 21:52:07 -0500
categories: jekyll update
---
{% include mathjax.html %}

## Chater 1 : Intro

* Definitions
  * feature measurements
  * outcome measurements
    * quantitative (continuous)
    * qualitative (categorical or discrete variables, also a.k.a. factors)
      * nominal
      * ordinal
  * learner : a prediction model
  * supervised learning : outcome variable is present
  * unsupervised learning : only features are present. Goal is to describe how data are organized or clustered
  * classification problems : when outcome is categorial
  * regression problems : when outcome is quantitative

Chapters 1-4 should be read in sequence. Chapter 7 is mandatory. Rest of the chapters can be sampled.

## Chapter 2 : Overview of Supervised Learning

* Definitions
  * inputs (machine learning) := predictors (statistical literature) := independent variables (classic literature) := features (pattern recognition literature)
  * outputs (machine learning) := responses (pattern recognition literature) := dependent variables (classic literature)
* Notations
  * General reference to variables
    * Input variable : $X$, either scalar or vector
    * Output variable : $Y$ if quantitative, or $G$ if qualitative
  * Component of vector $V$: $V_j$
  * Matrix : $\mathbf{X}$
  * an observation of $X$ : $x_i$, either scalar or vector
  * all observations on variable $X_j$ : $\mathbf{x_j}$

### Linear Regression (2.3.1)

How do we predict?

$$
\hat{Y} = X^{T}\beta
$$

How do we fit?

Popular method - Least squares: minimize residual sum of squares

$$ \text{RSS}(\beta) = \sum_{i=1}^{N} (y_i - x_i^T\beta)^2 $$

Or rewriting in matrix form

$$ \text{RSS}(\beta) = (\mathbf{y} - \mathbf{X}\beta)^T(\mathbf{y} - \mathbf{X}\beta) $$

It's quadratic so the minimum always exists. Thus, we can differentiate w.r.t. $\beta$, we get the _normal equation_ (i.e. closed form analytical solution to linear regression model with least squarescost function)

$$
\mathbf{X}^T(y-\mathbf{X}\beta) = 0
$$

If $\mathbf{X}^T\mathbf{X}$ is nonsingular (i.e. usually with $n > p$, columns must be independent), the unique solution is

$$
\hat{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
$$

Note that the entire fitted surface is characterized by the $p$ parameters of $\hat{\beta}$

### Linear Classification (2.3.1)

You can apply linear models to classification. For binary classification, $Y$ can be coded into 0 or 1, and the decision boundary can be {$x: x^T\hat{\beta}$=0.5}. Note that for $x^T\hat{\beta}=0$, the boundary is a hyperplane (a subspace whose dimension is one less than its ambient space), going through the origin and orthogonal to $\hat{\beta}$. And $x^T\hat{\beta}=c$ is such hyperplane shifting parallel by $\frac{c}{\hat{\lVert\beta\rVert}}$. This makes sense because $x$ would be vectors such that the inner product along the direction and magnitude of $\hat{\beta}$ would equal $c$.

### Two example sets of data with different characteristics

Let's say the context is binary classification with two classes, BLUE and ORANGE. Consider two different ways to sample the data.

1. Scenario 1: BLUE is sampled from bivariate gaussian $N(\mu_{BLUE}, c_{BLUE}I)$, and ORANGE is sampled from $N(\mu_{ORANGE}, c_{ORANGE}I)$. Note that covariances are zero.
2. Scenario 2: BLUE is sampled from a mixture of 10 gaussians. ORANGE is also sampled from a mixture of 10 gaussians, but from that characterized with a different parameter.

One way to describe mixture of Gaussians is to describe it as a generative model. First, generate a discrete variable that chooses which Gaussian to use (out of 10). Then, sample the data from the chosen Gaussian.

For Scenario 1, i.e. one Gaussian per class, linear decision boundary is optimal. We will show this in chapter 4. For Gaussian mixture model, a lienar boundary is not optimal. Instead, the optimal boundary should be nonlinear and disjoint.

### Nearest neighbor models (2.3.2)

Both prediction and the fit for k-nearest neighbor model is defined as

$$
\hat{Y}(x) = \frac{1}{k}\sum_{x_i\in N_k(x)}y_i
$$

where $N_k(x)$ is the neighborhood of $x$ defined by the $k$ closest points in $x_i$ in the training set. "Closeness" implies a metric, which we'll just use the euclidean distance for now. 

Note that although this model appears to have a single parameter, $k$, the _effective_ number of parameters is $N/k$.

### Linear least squares vs Nearest neighbors

We can view the two models as two extremes on the spectrum. 

* Linear model
  * Yields smooth, stable fits.
  * Relies heavily on the assumption that a linear model is appropriate.
  * Has low variance, and high bias.
  * More appropriate for Scenario 1
* Nearest neighbors
  * No assumptions about the underlying data, and can adapt to any situation.
  * Fits are unstable because each fitted parameter depends on a handful of input points.
  * High variance and low bias. 
  * More appropriate for Scenario 2

Most popular techniques today are variants of these two simple procedures. In fact 1-nearest-neighbor, the simplest of all, captures a large percentage of the market share for low-dimensional problems.

### Statistical Decision Theory (2.4)

This section is theoretical, and provides framework for developing models such as those discussed informally so far.

Let's think in terms of random variables and probability places. Let's first look at the quantitative case. This theory requires a _loss function_, and let's first look at the _squared error_ loss, i.e., $L(Y,f(X)) = (Y-f(X))^2$. The _expected prediction error_, computed with our choice of loss function, is 

$$
\begin{align}
EPE(f) &= E_{X,Y}[(Y-f(X))^2] \\
&= \int\int [y-f(x)]^2 Pr(x, y) dxdy \\
\end{align}
$$

Using $Pr(X,Y)=Pr(Y\lvert X)Pr(X)$,

$$
\begin{align}
EPE(f) &= \int\int [y-f(x)]^2 Pr(y\lvert X=x)Pr(x) dxdy \\
\end{align}
$$

Changing the order of summation, and factoring $Pr(x)dx$ out,

$$
\begin{align}
EPE(f) &= \int\int [y-f(x)]^2 Pr(y\lvert X=x)dyPr(x)dx \\
&= E_XE_{Y\lvert X}[(Y-f(X)^2\lvert X] \\
\end{align}
$$

(Note that I just applied the reverse of law of total expectation) And we can see that we can minimize the whole thing by minimize the inner loop for each given $x$, i.e. minimize EPE _pointwise_.

Since $E[(V-v)^2] = E[V^2] - 2vE[V] + v^2$ is minimized via $v=E[V]$ (take the derivative of $v$ on the RHS. FWIW, the minimum value becomes the $Var(V)$)

The solution is 

$$
f(x) = E(Y\lvert X=x)
$$

And FWIW, it naturally follows that the EPE(f) becomes

$$
EPE(f) = E_X[Var(Y\lvert X)]
$$

which is the "unexplained" variance from _law of total variance_ below. (First term is unexplained variance, and second term is explained variance). Note that the randomness comes thru $X$. Expection of $Y$ is constant (i.e. not random) given $X$. Viewing  $E[Y\lvert X]$ as a random variable, $E_{X,Y}[E[Y\lvert X]]=E_{X}[E[Y\lvert X]]$.

$$
Var(Y) = E_X[Var[Y\lvert X]] + Var_X[E[Y\lvert X]]
$$

In any case, our solution for $f(x)$ is called the _regression_ function. This is theoretically the best estimation of $Y$ given $X=x$, when the metric is squared error.

The nearest-neighbor method approximates the regression function with

$$
\hat{f}(x) = Avg(y_i | x_i \in N_k(x))
$$

Two approximations are happening here
* Expectation is approximated by averaging over sample data
* Conditioning is relaxed to conditioning over some region "close" to the target point $x$

Intuition: if $N$ is big, then it's likely to have more samples around your point $x$. Also, with larger $k$, the average will be more stable. Stating without proof, $\hat{f}(x) \to E[Y \lvert X=x]$ as $N,k \to \infty$ such that $k/N \to 0$.

Next question: if we have universal approximator, why even bother looking further? Few reasons
* Often we don't have that many samples
* Linear or more structured models can give you even more stable estimates, but you have to first make sure such models are more appropriate. You can learn such knowledge from the data.
* When the dimension $p$ gets large, so does the metric size of the nearest neighborhood. The convergence above holds, but the _rate_ of the convergence will decrease as dimension increase.

What approximations are happening for linear models?
* It just subsets your function space by $\beta^T x$, parameterized by $\beta$. Softly speaking, you assume that the true relationship is linear. (This is called _model-based approach_)
* So to find optimal $\hat{f}$, you can only optimize through $\beta$
* Further, if we are using least squares, we approximate the expectation (in the math below) with average

$$
\begin{align}
EPE(f) &= EPE(\vec\beta) & & \text{I'll make it explicit that beta is a vector where it needs to be}\\
&= E_{x_1, ..., x_p, y}[(y-(\sum x_i\beta_i))^2]  \\
&= E_{\vec x,y}[(y - \vec x^T \vec\beta)^2] & & \text{$x$ is just a random vector, so let's use vector notations for convenience} \\
\end{align}
$$

remember, matrix differentiation is just an invented/promised notation so that the result will be the Jacobian matrix of the element-wise differentiation 

$$
\begin{align}
\vec 0 &=\frac{\partial}{\partial{\vec\beta}}  E_{\vec x,y} [-2y(\vec x^T \beta) + (\vec x^T \beta)^2 ] & &  \text{Expectation is just a linear operation. So is a derivative, so we can just cascade the two}  \\
&= \frac{\partial}{\partial{\vec\beta}}  E_{\vec x,y} [-2y(\vec x^T \beta) + \vec\beta^T \vec x \vec x^T \vec\beta ] & & \text{just refactoring second term so that it's cannonical quadratic form} \\ 
&= E_{\vec x,y} [-2y \vec x^T + 2\vec\beta^T \vec x \vec x ^T] & & \text{just follows from common back of the envelope matrix differentiation rules} \\
E_{\vec x,y} [y \vec x^T] &= \vec \beta^T E_{\vec x} [\vec x \vec x^T] \\
\vec \beta &= E_{\vec x}[\vec x \vec x^T]^{-1} E_{\vec x, y}[y x] & & \text{just take the transpose on both sides, and use inverse}  \\
\end{align}
$$

Note that since we took a vector differentiation (pointwise differentiation for each $\beta_i$, the output is also element-wise matrix of expectation (in case a matrix in side an expectation is confusing).

Even though k-nearest neighbors and linear least squares are approximating conditional expectations with averages, there are key differences in model assumptions. 
* Linear least squares assumes that true $f(x)$ is well approximated by a globally linear function
* k-nearest neighbors assumes $f(x)$ is well approximated by a locally constant function

A lot of the techniques in this book will be model-based. This means we will impose model assumptions, which can replace conditional expectation in the final derived solution (For example, our solution for $\hat{\beta}$ no longer involves conditioning on X). 

What if we use $L_1$ loss function instead of $L_2$? The solution, stating without proof, is the conditional median


$$
\hat{f}(x) = median(Y\lvert X=x)
$$

Median is a different measure of _location_ (i.e. typical or central value that best describes data), and its estimates are more robust than the condtiional mean. However, $L_1$ criteria has discontinuities in their derivatives, so $L_2$ is more analytically convenient and widely used.

The same paradigm works for qualitative case. The most often used loss function is the _zero-one_ loss function. Denoting the categorial output variable as $G$, the solution becomes

$$
\hat{G}(x) = max_{g \in G} Pr(g \lvert X = x)
$$

This is known as the _Bayes classifier_, which says we classify to the most probable class using $Pr(G\lvert X)$. The error rate of the Bayes classifier is called the _Bayes rate_. If the distribution $Pr(G\lvert X)$ is known, then the classifier / decision boundary can be calculated exactly.

The most popular way of encoding a $K$-level qualitative variable is to use a one-hot vector of length $K$, known as the _dummy variable_. Under the one-hot scheme, and assuming we are dealing more than simple binary classification, output $Y$ is now a vector. Since $Y_k=1$ given $G=\mathcal{G}_k$, it follows that $E[Y_k \lvert X] = Pr(G = \mathcal{G}_k \lvert X)$. That means if we classify $G$ with $G_k$ where $k$ is the coordinate with the largest value $Y_k$ within the vector, the dummy variable + regression paradigm gives exactly the same output as the Bayes classifier.

### Local methods in high dimensions (2.5)

#### Curse of dimensionality

Local methods ($k$-nearest neighbors is an example of local method) suffer from the curse of dimensionality
* Imagine samples are distributed in $p$-dimensional unit hypercube. Taking $p=10$ dimension, $0.8^{10}=0.1$, i.e., you must cover 80% of the range of input variable to capture 10% of the data to form a local average. Such neighborhoods are no longer local.
* Imagine samples are distributed in $p$-dimensional unit ball. At the origin, the median distance from the origin to closest data point becomes farther as $p$ increases. That means more data points are closer to the boundary of the sample space than to any other point. The reason this presents a problem is that prediction is more difficult near the edges of the training sample. One must extrapolate from neighboring sample points rather than interpolate between them.

Generally, you can think that the density of the samples (i.e. sampling density) is proportional to $N^{1/p}$. That means if you have $p=1$ and $N=100$ for a single input problem, you need $N=100^{10}$ samples if you want to achieve the same density with $p=10$. Therefore, in high dimension, all realistic/feasible real world datasets populate the input space sparsely.

#### Bias-variance decomposition

Bias-variance decomposition is a decomposition of the mean squared error _given a specific input point $x$_. (i.e. $MSE$ is a function of $x$. We're evaluating this pointwise as we did above!). We'll say there's a true deterministic function $y=f(x)$. We abbreviate $f(x)=f$. $\mathcal{T}$ denotes the training set.

$$
\begin{align}
MSE(x) &= E_{\mathcal{T}}[(\underbrace{y}_{\text{function of $x$}}-\underbrace{\hat{y}^2}_{\text{the fitted predictor is function of $\mathcal{T}$, and the prediction is function of $x$}}]  \\
&= E_{\mathcal{T}} [ (f-\hat{y})^2 ] \\
&= E_{\mathcal{T}} [ f^2 - 2f\hat{y} + \hat{y}^2 ] \\
&= f^2 - 2f E_{\mathcal{T}} [ \hat{y}^2 ] + E_{\mathcal{T}}[\hat{y}^2] \;\;\; \text{we already said $x$ is an input, and that $f(x)$ is deterministic. } \\
&= f^2 - 2f E_{\mathcal{T}} [ \hat{y}^2 ] + \underbrace{E_{\mathcal{T}}[\hat{y}^2] - (E_{\mathcal{T}}[\hat{y}])^2}_{Var_{\mathcal{T}}(\hat{y})} + (E_{\mathcal{T}}[\hat{y}])^2\\
&= Var_{\mathcal{T}}(\hat{y}) + (E_{\mathcal{T}}[\hat{y}]-f)^2 \;\;\; \text{trivial from above} \\
&= Var_{\mathcal{T}}(\hat{y}) + Bias^2(\hat{y}) \;\;\;
\end{align}
$$

$Var_{\mathcal{T}}$ comes from the sampling variance and how sensitive your fits are to it.

Let's set up a new context where
* True relationship between $X$ and $Y$ is linear with an error $Y= X^T\beta + \epsilon$, and $\epsilon \sim N(0, \sigma^2)$
* We decide to fit the model by least squares

Let's redo the math for $EPE(x)$. I'm going to omit square brackets for expectation

$$
\begin{align}
EPE(x) &= E(y-\hat{y})^2 \\
& \text{First start with a general $E$, which means we intend to sum over all random variables the expression contains}\\
& \text{but at the moment we do not have the need to specify the specific distribution or the random variable that}\\
& \text{causes the randomness}\\
&= E(y^2 - 2y\hat{y} + \hat{y}^2)\\
&= E(y^2 - 2y\hat{y} + \underbrace{\hat{y}^2 - (E\hat{y})^2}_{Var(\hat{y})} + (E\hat{y})^2)\\
& \text{Best trick in math : add and subtract the same thing}\\
&= E(y^2 -2y\hat{y} + (E\hat{y})^2) + Var(\hat{y})\\
& \text{Note that $Var(\hat{y})$ is a constant expression. so we can pull out from the expectation.}\\
&= Ey^2 - \underbrace{2E_{Y\lvert X}E_{\mathcal{T}}y\hat{y}}_{\text{$\hat{y}$ depends on the fit, which depends on $\mathcal{T}$. $y$ depends on $x$, but $x$ is given in LHS, so we should use $Y \vert X$.}} + (E\hat{y})^2 + Var(\hat{y})\\
& \text{Note that we just expanded $E$ into a "nested for loop"}\\
&= Ey^2 - 2E_{Y\lvert X}yE_{\mathcal{T}}\hat{y} + (E\hat{y})^2 + Var(\hat{y})\\
& \text{since $y$ doesn't depend on $\mathcal{T}$, we can pull it out to the outer expectation} \\
&= Ey^2 - (Ey)^2 + (Ey)^2 - 2E_{Y\lvert X}yE_{\mathcal{T}}\hat{y} + (E\hat{y})^2 + Var(\hat{y})\\
& \text{Best trick in math : add and subtract the same thing} \\
&= Var(y) + (Ey-E\hat{y})^2 + Var(\hat{y}) \\
&= \underbrace{Var_{Y \lvert x}(y)}_{\text{$x$ is given, and $y$ depends on $x$, so we should use $Y \lvert X$}} + Bias(\hat{y})^2 + Var(\hat{y}) \\
&= Var_{Y \lvert X}(y) + Var(\hat{y}) \\
& \text{We are just going to use without proof that the estimates are going to be unbiased for this setup}\\
&= \sigma^2 + E_{\mathcal{T}}x^T(\mathbf{X}^T\mathbf{X})^{-1}x\sigma^2\\
& \text{First term follows from $y=x^T \beta + \epsilon$. We'll see how second term was derived in Chapter 3}.\\
\end{align}
$$

We now know what $EPE(x)$ is. Then what is $E[EPE]$, which is $E_{x}[EPE(x)]$? First, we have to remind ourselves that _trace_ is a useful tool analyzing the distribution of quadratic forms.

Lemma: let $x$ a random vector. Then

$$
\begin{align}
E[x^T A x] &= tr(E[x^T A x])   \\
& \text{since quadratic form is a scalar quantltiy, LHS and RHS is a scalar}\\
&= E[tr(x^T A x)] \\
& \text{Trace is a linear operator because it's a linear combination of components of the matrix.}\\
& \text{Expectation is also a linear operator, this allows us to interchange the order of trace and expectation}\\
&= E[tr(A x x^T)] \\
& \text{this uses the cyclic property of trace}\\
&= tr(A E[x x^T])\\
&= tr(A ( Cov(x) + E[x]E[x^T] ))\\
& \text{This just follows from the standard properties of variance and covariance}\\
&= tr(A Cov(x)) + E[x^T] A E[x] ))\\
& \text{applying the cyclic property one more time}\\
\end{align}
$$

For convenience, let's say $E[x]=0$, (i.e. $x$ is centered). Then, the final expected $EPE$ becomes

$$
\begin{align}
E[EPE] &= E_{x}{EPE(x)}\\
&= E_{x}[\sigma^2 + E_{\mathcal{T}}x^T(\mathbf{X}^T\mathbf{X})^{-1}x\sigma^2]\\
& \sim \sigma^2 + E_{x} [ x^T Cov(X)^{-1} x \sigma^2 / N] \\
& \text{Using $\mathcal{T}^T \mathcal{T} \to Cov(X) / N$ with large $N$ }\\
&= \sigma^2 + tr(Cov(X)^{-1} Cov(x)) \sigma^2 / N\\
& \text{Using $E[x]=0$ and the Lemma above}\\
&= \sigma^2 + p \sigma^2 / N\\
\end{align}
$$

We have shown that in this context, expected $EPE$ increases linearly as a function of $p$. Wait, why does error increase with $p$? Isn't there supposed to be a sweet spot for $p$? Should we just use $p=1$ all the time?

No, since for this contrived example, $p$ also affects the _true_ function $y=x^T\beta$, and we use the same $p$ for true function as well as modeling. Note in our previous analysis, curse of dimensionality makes the error increase exponentially with $p$. With the rigid least squares model, it only increases linearly. 

Also, while least squares model is unbiased in this setup because the true function is linear. It can be shown the least squares model can have bias when the function is something like $y=x^3$. **If true function is indeed linear, then we have removed the bias to zero, and hence reducing EPE. However, if the assumptions are wrong (i.e. the true function is far from linear) EPE of linear model can be dominated by the bias, to the extent that it's worse than nearest neighbor models**.  We'll learn a whole spectrum of models between the rigid linear models and the extremely flexible 1-nearest-neighbor models, each with their own assumptions and biases, which have been proposed specifically to avoid the exponential growth in complexity of functions in high dimensions by drawing heavily on these assumptions about any special structure that is known to exist.

### Statistical models, Supervised learning and function approximation (2.6)

This subchapter provides a high level narrative how to view the statistical modeling problem. Our goal is to find an approximation $\hat{f}(x)$ for $f(x)$.

For most systems, $Y$ and $X$ do not have deterministic relationship. A simple way to model the non-deterministic relationship is through the _additive error model_.

$$
Y = f(X) + \epsilon
$$

In reality, there will be other unmeasured variables that also contribute to $Y$, including the _measurement error_. The additive error model assumes we can approximate all randomness via the error $\epsilon$.

Note that problems with deterministic relationship exist. Many of classification problems studied in machine learning are of this form. The response surface can be thought of as a colored map defined in $\mathcal{R}^p$. The goal is to be able to color any point. Hence the function is deterministic, and the randomness enters through the x location of the training points. We will not talk about such problems right now.

Assuming that $\epsilon$ has $E[\epsilon]=0$ and is independent of $X$, in this simple model, $Pr(Y \lvert X=x)$ is just a distribution centered at $f(x)$, with variance of $\sigma(\epsilon)$. However, we can make simple modifications to avoid the independence assumption. For example, we can have $Var(Y \lvert X=x) = \sigma(x)$, and replace whenever that term is used in closed form.

#### Function approximation

Machine learning has analogies to human learning (learning algorithm adjusts itself after iterations of example), and human brain (neural network). However, this book approaches the function-fitting paradigm using approaches taken in applied mathematics and statistics, which is via function approximation and estimations. In this paradigm, the data pairs $\( x_i, y_i\)$ are viewed as points in a (p+1)-dimensional Euclidean space. The function domain is $\mathcal{R}^p$, and the goal is to obtain an approximation to $f(x)$ for all $x$ in some region of $\mathcal{R}^p$. Treating supervised learning as a problem in function approximation encourages geometrical concepts of Euclidean spaces and mathematical concepts of probabilistic inference to be applied to the problem. 

Many of the approximations we will encounter have associated a set of parameters $\theta$ that can be modified to be fit to the data. For the linear model, we get a simple closed form solution to the minimization problem with the RSS. Even if we are using _lienar basis expansions_ model (see 2.30), this will be true. However, if the basis functions have any hidden parameters, the solution requires either iterative methods or numerical optimization.

RSS is just a special case criterion, and a more general principle for estimation would be using MLE, which minimizes the log-probability for the obsered samples.

$$
L(\theta) = \sum_{i=1}^N log Pr_{\theta}(y_i)
$$

For the additive error model, the solution for MLE would be the exact same solution for RSS, as both would have to minimize $\sum_{i-1}^N (y_i - f_{\theta}(x_i))^2$.

As I thought about this more, I realized that linear regression is just (rigidly) assuming the conditional probability is $Pr_{\theta}(Y \lvert X=x) = \mathcal{N}(\theta^T x, \sigma(\epsilon))$, and tries to fit the MLE. A discriminative model tries to learn the $Pr(Y \lvert Y)$, so seems like we're just sticking to this for now.

For qualitative cases, the log-likelihood would be summing over the multinomial likelihood

$$
L(\theta) = \sum_{i=1}^{N} log Pr(G=\mathcal{G}_k \lvert X=x_i)
$$

### Structured Regression Models (2.7)

Consider we are fitting a function using the RSS criterion.

$$
RSS(\hat{f}) = \sum_{i=1}^{N} (y_i - \hat{f}(x_i))^2
$$

Minimizing the above leads to infinitely many solutions for $\hat{f}$. Any $\hat{f}$ that passes through all data points is a valid optimal solution! 

We must restrict the eligible solutions to a smaller set of functions. These restrictions are sometimes encoded via the parametric representation of $f_{\theta}$, or may be built into the learning method itself, either implicitly or explicitly. These restricted classes of solutions are the major topic of this book.

To be clear, any restrictions imposed on $\hat{f}$ that lead to unique solution do not remove the ambiguity caused by the multiplicity of solutions. There are infinitely many possible restrictions, so the ambiguity has simply transferred to the choice of constraint. 

The constraints imposed by most learning tools usually imposes some kind of regular/homogeneous behavior in small neighborhoods of the input space. That means for all input points $x$ sufficiently close to each other in some metric, $\hat{f}$ exhibits some special structure such as constant, linear, or low-order polynomial behavior.

The strength of the constraint is dictated by the neighborhood size. For example, local lienar fits in very large neighborhoods is almost a globally linear model, which is very restrictive.

The book seems to be suggest that the concept of "constraint" is equivalent to the concept of "neighborhood". That means the constraint (neighborhood) depends on the metric used. Methods like kernel, local regression, and tree-based methods directly specify the metric and size of the neighborhood. Other methods like splines, neural networks, and basis-function methods implicitly define neighborhoods of local behavior.

One fact does not change. Any tool that tries to small isotropic (Formal meaning: same value measured in different directions. In this context: treats all dimension as the same) neighborhood will face the curse of dimensionality. Conversely, all tools that overcome the dimensionality problem have an associated (implicit or adaptive) metric for measuring neighborhoods, which basically does not allow the neighborhood to be simultaneously small in all directions. 

References
* [Matrix differentiation][matrix-calculation] 
* [Intuition behind law of total variance][law-of-total-variance]

[matrix-calculation]: {{ site.baseurl }}/pdfs/MatrixCalculus.pdf
[law-of-total-variance]: https://math.stackexchange.com/questions/1742578/law-of-total-variance-intuition

